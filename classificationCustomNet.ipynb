{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMGamWk9j5VrIBpZ3QD2W9/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brutusa/python-machine-learning/blob/main/classificationCustomNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# By Andujar Brutus\n",
        "from math import exp\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Import data\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "cancer_categories = cancer.target_names\n",
        "cancer_features = cancer.feature_names\n",
        "\n",
        "# Split data (maitain specific data split)\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.70, test_size=.30, random_state=20)\n",
        "\n",
        "# Split data (randomly organize data split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.70, test_size=.30)\n",
        "\n",
        "# Get data's dimensions\n",
        "X_train_num_instances = X_train.shape[0]\n",
        "X_train_num_features = X_train.shape[1]\n",
        "\n",
        "X_test_num_instances = X_test.shape[0]\n",
        "X_test_num_features = X_test.shape[1]\n",
        "\n",
        "y_train_num_instances = y_train.shape[0]\n",
        "\n",
        "y_test_num_instances = y_test.shape[0]\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.05\n",
        "\n",
        "# Bias Neuron\n",
        "bias_neuron = 1\n",
        "\n",
        "# Set starting arrays to all zeroes\n",
        "parameter_vector = np.zeros((X_train_num_features + 1), dtype=np.float128)\n",
        "train_instance_factor = np.zeros((X_train_num_instances), dtype=np.float128)\n",
        "logistic_regression_array = np.zeros((X_train_num_instances), dtype=np.float128)\n",
        "h_theta_array = np.zeros((X_test_num_instances), dtype=np.float128)\n",
        "predicted_class = np.zeros((X_test_num_instances), dtype=np.int64)\n",
        "test_instance_factor = np.zeros((X_test_num_instances), dtype=np.float128)\n",
        "\n",
        "# Dataset copy for use with bias\n",
        "X_train_bias = np.zeros((X_train_num_instances, X_train_num_features), dtype=np.float128)\n",
        "X_test_bias = np.zeros((X_test_num_instances, X_train_num_features), dtype=np.float128)\n",
        "np.copyto(X_train_bias, X_train)\n",
        "np.copyto(X_test_bias, X_test)\n",
        "\n",
        "y_train_bias = np.zeros((y_train_num_instances), dtype=np.float128)\n",
        "y_test_bias = np.zeros((y_test_num_instances), dtype=np.float128)\n",
        "\n",
        "np.copyto(y_train_bias, y_train)\n",
        "np.copyto(y_test_bias, y_test)\n",
        "\n",
        "# Add bias nueron to front of datasets\n",
        "X_train_bias = np.insert(X_train_bias, 0, bias_neuron, axis=1)\n",
        "X_test_bias = np.insert(X_test_bias, 0, bias_neuron, axis=1)\n",
        "\n",
        "# Get dimensions of new datasets\n",
        "X_train_bias_num_instances = X_train_bias.shape[0]\n",
        "X_train_bias_num_features = X_train_bias.shape[1]\n",
        "\n",
        "X_test_bias_num_instances = X_test_bias.shape[0]\n",
        "X_test_bias_num_features = X_test_bias.shape[1]\n",
        "\n",
        "y_train_bias_num_instances = y_train_bias.shape[0]\n",
        "\n",
        "y_test_bias_num_instances = y_test_bias.shape[0]\n",
        "\n",
        "\n",
        "# Normalize a full vector\n",
        "def normalize(v, min, max):\n",
        "  # Min Max Normalization formula\n",
        "  new_v = (v - min)/ (max-min)\n",
        "  return new_v\n",
        "\n",
        "# Normalize dataset when called\n",
        "def normalize_vectors():\n",
        "  # Normalize the training vector\n",
        "  for instance_X in range(X_train_num_instances):\n",
        "    # normalize everything from index 1 to end (skipping bias neuron)\n",
        "    for feature_X in range(1, X_train_num_features + 1):\n",
        "      X_train_bias[instance_X, feature_X] = normalize(X_train_bias[instance_X, feature_X], np.min(X_train_bias), np.max(X_train_bias))\n",
        "\n",
        "  # Normalize the testing vector\n",
        "  for instance_X in range(X_test_num_instances):\n",
        "     # normalize everything from index 1 to end (skipping bias neuron)\n",
        "    for feature_X in range(1, X_test_num_features + 1):\n",
        "      X_test_bias[instance_X, feature_X] = normalize(X_test_bias[instance_X, feature_X], np.min(X_test_bias), np.max(X_test_bias))\n",
        "  return None\n",
        "\n",
        "## LOGISTIC REGRESSION ALGORITHM\n",
        "# Train the model\n",
        "def train_logistic_regression():\n",
        "  # For every example in our training dataset\n",
        "  for instance_X in range(X_train_num_instances):\n",
        "  # And for every feature\n",
        "    for feature_X in range(X_train_num_features+1):\n",
        "      # 1st: Calculate Theta-Transposed X for each instance\n",
        "      train_instance_factor[instance_X] += (parameter_vector[feature_X] * X_train_bias[instance_X, feature_X])\n",
        "\n",
        "    # And for every feature after adding up the factor\n",
        "    for feature_X in range(X_train_num_features+1):\n",
        "      # 2nd: Calculate gradient descent for every instance and for every feature\n",
        "      logistic_regression_array[instance_X] = ((1/(1 + (np.exp(-(train_instance_factor[instance_X]))))) - y_train[instance_X]) * X_train_bias[instance_X, feature_X]\n",
        "\n",
        "      # 3rd: Compute full formula for every feature\n",
        "      parameter_vector[feature_X] = parameter_vector[feature_X] - (learning_rate*(np.sum(logistic_regression_array[instance_X])))\n",
        "  return None\n",
        "\n",
        "\n",
        "# Predict the targets\n",
        "def test_logistic_regression():\n",
        "  # For every example in our training dataset\n",
        "  for instance_X in range(X_test_num_instances):\n",
        "    # And for every feature\n",
        "    for feature_X in range(X_test_num_features+1):\n",
        "     # 1st: Calculate Theta-Transposed X for each instance\n",
        "      test_instance_factor[instance_X] += (parameter_vector[feature_X] * X_test_bias[instance_X, feature_X])\n",
        "    # And for every feature\n",
        "    for feature_X in range(X_test_num_features+1):\n",
        "      # 2nd: Calculate the target output\n",
        "      h_theta_array[instance_X] = 1/(1 + (np.exp(-(test_instance_factor[instance_X]))))\n",
        "\n",
        "      # 3rd: Predict class\n",
        "      if h_theta_array[instance_X] >= 0.5:\n",
        "        predicted_class[instance_X] = 1\n",
        "      else:\n",
        "        predicted_class[instance_X] = 0\n",
        "  return None\n",
        "\n",
        "# Get prediction accuracy for logistic regression\n",
        "def get_logistic_accuracy():\n",
        "  correct = 0\n",
        "  for instance_X in range(X_test_num_instances):\n",
        "    if predicted_class[instance_X] == y_test[instance_X]:\n",
        "      correct += 1\n",
        "  return (correct/len(predicted_class))*100\n",
        "\n",
        "#-------------------------------------------------------------------------------------\n",
        "\n",
        "## FEED FORWARD NEURAL NETWORK ##\n",
        "\n",
        "# 1. Initialize the weights and biases\n",
        "# Array SHx is the hidden layer array which stores all the weights from inputs x\n",
        "# b1 bias is within each individual hidden layer array at index 0\n",
        "b2_bias_weight = (1 - (-1)) * np.random.random_sample() + (-1)\n",
        "\n",
        "SH1wx = np.zeros((X_train_bias_num_features), dtype=np.float128)\n",
        "SH2wx = np.zeros((X_train_bias_num_features), dtype=np.float128)\n",
        "SH3wx = np.zeros((X_train_bias_num_features), dtype=np.float128)\n",
        "SH4wx = np.zeros((X_train_bias_num_features), dtype=np.float128)\n",
        "target_output = None\n",
        "\n",
        "for feature_X in range(X_train_bias_num_features):\n",
        "  SH1wx[feature_X]= (1 - (-1)) * np.random.random_sample() + (-1)\n",
        "  SH2wx[feature_X]= (1 - (-1)) * np.random.random_sample() + (-1)\n",
        "  SH3wx[feature_X]= (1 - (-1)) * np.random.random_sample() + (-1)\n",
        "  SH4wx[feature_X]= (1 - (-1)) * np.random.random_sample() + (-1)\n",
        "\n",
        "# SOx is output and wx is the weight from the hidden layer (between -1 and 1)\n",
        "SO1wx = np.zeros(5, dtype=np.float128)\n",
        "\n",
        "for hidden_weight_x in range(5):\n",
        "  SO1wx[hidden_weight_x]= (1 - (-1)) * np.random.random_sample() + (-1)\n",
        "\n",
        "# 2. Feed the training sample\n",
        "#Hidden Layer calculations\n",
        "def SH1_calc(dataset):\n",
        "  SH1_sum = 0\n",
        "  for instance_X in range(len(dataset)):\n",
        "    for feature_X in range(len(dataset[0])):\n",
        "      SH1_sum += (X_train_bias[instance_X, feature_X] * SH1wx[feature_X])\n",
        "    return SH1_sum\n",
        "\n",
        "def SH2_calc(dataset):\n",
        "  SH2_sum = 0\n",
        "  for instance_X in range(len(dataset)):\n",
        "    for feature_X in range(len(dataset[0])):\n",
        "      SH2_sum += (X_train_bias[instance_X, feature_X] * SH2wx[feature_X])\n",
        "    return SH2_sum\n",
        "\n",
        "def SH3_calc(dataset):\n",
        "  SH3_sum = 0\n",
        "  for instance_X in range(len(dataset)):\n",
        "    for feature_X in range(len(dataset[0])):\n",
        "      SH3_sum += (X_train_bias[instance_X, feature_X] * SH3wx[feature_X])\n",
        "    return SH3_sum\n",
        "\n",
        "def SH4_calc(dataset):\n",
        "  SH4_sum = 0\n",
        "  for instance_X in range(len(dataset)):\n",
        "    for feature_X in range(len(dataset[0])):\n",
        "      SH4_sum += (X_train_bias[instance_X, feature_X] * SH4wx[feature_X])\n",
        "    return SH4_sum\n",
        "\n",
        "# 3. Propagate inputs forward. Compute net input & output of each unit in hidden + output layers\n",
        "# Activation function\n",
        "def sigmaS(SHx_sum):\n",
        "  return (1 / (1 + np.exp(-(SHx_sum))))\n",
        "\n",
        "# Calculated separately for ease of use\n",
        "def sigmaSH1(dataset):\n",
        "  return sigmaS(SH1_calc(dataset))\n",
        "\n",
        "def sigmaSH2(dataset):\n",
        "  sigmaSH2 = sigmaS(SH2_calc(dataset))\n",
        "  return sigmaSH2\n",
        "\n",
        "def sigmaSH3(dataset):\n",
        "  sigmaSH3 = sigmaS(SH3_calc(dataset))\n",
        "  return sigmaSH3\n",
        "\n",
        "def sigmaSH4(dataset):\n",
        "  sigmaSH4 = sigmaS(SH4_calc(dataset))\n",
        "  return sigmaSH4\n",
        "\n",
        "# Output layer\n",
        "def netO1(dataset):\n",
        "  return ((SO1wx[0]*bias_neuron) + (SO1wx[1]*sigmaSH1(dataset)) + (SO1wx[2]*sigmaSH2(dataset)) + (SO1wx[3]*sigmaSH3(dataset)) + (SO1wx[4]*sigmaSH4(dataset)))\n",
        "\n",
        "# Final output layer value\n",
        "def sigma_netO1(dataset):\n",
        "  return sigmaS(netO1(dataset))\n",
        "\n",
        "# set target value\n",
        "def set_target(target):\n",
        "  global target_output\n",
        "  target_output = target\n",
        "\n",
        "# get target value\n",
        "def get_target():\n",
        "  return target_output\n",
        "\n",
        "# Output Layer Error (Error K)\n",
        "def output_layer_error(output_result):\n",
        "  return output_result * (1 - output_result) * (get_target() - output_result)\n",
        "\n",
        "# Output Layer Error terms\n",
        "def o1_output_error(dataset):\n",
        " return output_layer_error(sigma_netO1(dataset))\n",
        "\n",
        "def b2_bias_error(dataset):\n",
        "  return o1_output_error(dataset) * SO1wx[0]\n",
        "\n",
        "def SH1SO1_error(dataset):\n",
        "  return o1_output_error(dataset) * SO1wx[1]\n",
        "\n",
        "def SH2SO1_error(dataset):\n",
        "  return o1_output_error(dataset) * SO1wx[2]\n",
        "\n",
        "def SH3SO1_error(dataset):\n",
        "  return o1_output_error(dataset) * SO1wx[3]\n",
        "\n",
        "def SH4SO1_error(dataset):\n",
        "  return o1_output_error(dataset) * SO1wx[4]\n",
        "\n",
        "# Hidden Layer Errors (Error J)\n",
        "def hidden_layer_error(SHx_out, output_error, weight):\n",
        "  error = SHx_out * (1 - SHx_out) * output_error * weight\n",
        "  return error\n",
        "\n",
        "def H1_output_error(dataset):\n",
        "  error = 0\n",
        "  # bias updates within each array at index 0\n",
        "  for feature_X in range(X_train_bias_num_features):\n",
        "    error = hidden_layer_error(sigmaSH1(dataset), o1_output_error(dataset), SH1wx[feature_X])\n",
        "  return error\n",
        "\n",
        "def H2_output_error(dataset):\n",
        "  error = 0\n",
        "  # bias updates within each array at index 0\n",
        "  for feature_X in range(X_train_bias_num_features):\n",
        "    error = hidden_layer_error(sigmaSH2(dataset), o1_output_error(dataset), SH2wx[feature_X])\n",
        "  return error\n",
        "\n",
        "def H3_output_error(dataset):\n",
        "  error = 0\n",
        "  # bias updates within each array at index 0\n",
        "  for feature_X in range(X_train_bias_num_features):\n",
        "    error = hidden_layer_error(sigmaSH3(dataset), o1_output_error(dataset), SH3wx[feature_X])\n",
        "  return error\n",
        "\n",
        "def H4_output_error(dataset):\n",
        "  error = 0\n",
        "  # bias updates within each array at index 0\n",
        "  for feature_X in range(X_train_bias_num_features):\n",
        "    error = hidden_layer_error(sigmaSH4(dataset), o1_output_error(dataset), SH4wx[feature_X])\n",
        "  return error\n",
        "\n",
        "# Update weights between hidden and output layer formula\n",
        "def delta_jk(output_layer_error, hidden_layer_neuron):\n",
        "  return learning_rate * output_layer_error * hidden_layer_neuron\n",
        "\n",
        "def update_output_layer_weights(dataset):\n",
        "  # update weights between hidden and output layers + the bias\n",
        "  SO1wx[0] += (learning_rate * o1_output_error(dataset))\n",
        "\n",
        "  SO1wx[1] += delta_jk(SH1SO1_error(dataset), sigmaSH1(dataset))\n",
        "  SO1wx[2] += delta_jk(SH2SO1_error(dataset), sigmaSH2(dataset))\n",
        "  SO1wx[3] += delta_jk(SH3SO1_error(dataset), sigmaSH3(dataset))\n",
        "  SO1wx[4] += delta_jk(SH4SO1_error(dataset), sigmaSH4(dataset))\n",
        "  return None\n",
        "\n",
        "# Update weights between input and hidden layer formula\n",
        "def delta_ij(hidden_layer_error, sigma_SHx):\n",
        "  return learning_rate * hidden_layer_error * sigma_SHx\n",
        "\n",
        "def update_hidden_layer_weights(dataset):\n",
        "  # Update all weights between input and hidden layer + the bias\n",
        "  for instance_X in range(len(dataset)):\n",
        "    SH1wx[instance_X] += delta_ij(H1_output_error(dataset), sigmaSH1(dataset))\n",
        "    SH2wx[instance_X] += delta_ij(H2_output_error(dataset), sigmaSH2(dataset))\n",
        "    SH3wx[instance_X] += delta_ij(H3_output_error(dataset), sigmaSH3(dataset))\n",
        "    SH4wx[instance_X] += delta_ij(H4_output_error(dataset), sigmaSH4(dataset))\n",
        "    return None\n",
        "\n",
        "# Train ANN\n",
        "def train_ANN():\n",
        "  SH1_calc(X_train_bias)\n",
        "  SH2_calc(X_train_bias)\n",
        "  SH3_calc(X_train_bias)\n",
        "  SH4_calc(X_train_bias)\n",
        "\n",
        "  i = 0\n",
        "  while( i < len(X_train_bias)):\n",
        "    set_target(y_train_bias[i])\n",
        "    update_output_layer_weights(X_train_bias)\n",
        "    update_hidden_layer_weights(X_train_bias)\n",
        "    i += 1\n",
        "  return None\n",
        "\n",
        "# Test ANN\n",
        "def test_ANN():\n",
        "  correct = 0\n",
        "\n",
        "  for instance_X in range(X_test_bias_num_instances):\n",
        "    observed_output = 0\n",
        "    i = 0\n",
        "    #print(\"Instance  \", instance_X)\n",
        "    while(observed_output != y_test_bias[instance_X]):\n",
        "      set_target(y_test_bias[instance_X])\n",
        "      # print(\"epoch \", i)\n",
        "      # print(SO1wx)\n",
        "      update_output_layer_weights(X_test_bias)\n",
        "      update_hidden_layer_weights(X_test_bias)\n",
        "\n",
        "      if (sigma_netO1(X_test_bias)) >= .5:\n",
        "        observed_output = 1\n",
        "      else:\n",
        "        observed_output = 0\n",
        "      # used for debugging\n",
        "      # print(\"observed output is\", observed_output,\" and target output is \", get_target())\n",
        "      # print(\"OUTPUT VALUE IS \", sigma_netO1(X_test_bias))\n",
        "\n",
        "      i += 1\n",
        "\n",
        "\n",
        "      if observed_output == get_target():\n",
        "        correct += 1;\n",
        "\n",
        "      accuracy = (correct/len(y_test_bias))*100\n",
        "  return accuracy\n",
        "\n",
        "normalize_vectors()\n",
        "\n",
        "train_logistic_regression()\n",
        "test_logistic_regression()\n",
        "print(\"Logistic Regression Accuracy: {:.1f}\".format(get_logistic_accuracy()),\"%\")\n",
        "\n",
        "\n",
        "# The ANN may take a bit to process the data. please be patient.\n",
        "train_ANN()\n",
        "print(\"Feed Forward ANN Accuracy: {:.1f}\".format(test_ANN()),\"%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deBMrg3gRhug",
        "outputId": "7d9a2ca0-39e0-4a81-ab44-baae8f65700a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 68.4 %\n",
            "Feed Forward ANN Accuracy: 64.3 %\n"
          ]
        }
      ]
    }
  ]
}